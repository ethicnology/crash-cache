# =============================================================================
# CRASH-CACHE CONFIGURATION
# =============================================================================
# Quick setup profiles (uncomment one):
#   - SMALL:  1-5k users,  2GB RAM, 1 CPU   (dev/small prod)
#   - MEDIUM: 5-20k users, 8GB RAM, 4 CPU   (production)
#   - LARGE:  20k+ users,  16GB RAM, 8+ CPU (high traffic)
# =============================================================================

# =============================================================================
# DATABASE
# =============================================================================
# PostgreSQL connection string
DATABASE_URL=postgresql://username:password@localhost/crash_cache

# Connection pool size
# Controls how many concurrent database operations can run
# Rule of thumb: (expected_concurrent_requests × 0.3) with 50% headroom
#   SMALL:  10-15  |  MEDIUM: 30-40  |  LARGE: 80-100
DATABASE_POOL_SIZE=30

# Connection timeout (seconds)
# How long to wait for an available connection before returning 503
#   SMALL:  30  |  MEDIUM: 20  |  LARGE: 15
DATABASE_POOL_TIMEOUT_SECS=20

# =============================================================================
# HTTP SERVER
# =============================================================================
# Listen address and port
SERVER_HOST=0.0.0.0
SERVER_PORT=3000

# Maximum payload sizes (bytes)
# Use multiplication for readability: 50 * 1024 = 50KB, 200 * 1024 = 200KB
# Compressed: gzip-encoded payload from client
# Uncompressed: raw JSON after decompression
# Based on real data analysis: avg compressed ~4KB, max ~22KB | avg uncompressed ~16KB, max ~105KB
# Recommended: 50KB compressed / 200KB uncompressed (covers 100% of normal cases)
MAX_COMPRESSED_PAYLOAD_BYTES="50 * 1024"
MAX_UNCOMPRESSED_PAYLOAD_BYTES="200 * 1024"

# =============================================================================
# BACKGROUND WORKER (Digest/Processing)
# =============================================================================
# How often the digest worker runs (seconds)
# Also controls cache TTL for health stats and project validation
# Lower = faster processing, higher CPU usage
#   SMALL:  60  |  MEDIUM: 60  |  LARGE: 30
WORKER_INTERVAL_SECS=60

# How many queued reports to process per worker cycle
# Higher = faster processing but more memory usage
#   SMALL:  50  |  MEDIUM: 100  |  LARGE: 200
WORKER_REPORTS_BATCH_SIZE=100

# Note: Worker automatically stops at 90% of WORKER_INTERVAL_SECS
#       to prevent overlap between cycles (no configuration needed)

# =============================================================================
# CONCURRENCY & PERFORMANCE
# =============================================================================
# Maximum concurrent compression operations
# Compression is CPU-intensive, limit to prevent overload
# Rule of thumb: 2-3x CPU cores
#   SMALL:  4   |  MEDIUM: 8-12  |  LARGE: 16-24
MAX_CONCURRENT_COMPRESSIONS=12

# =============================================================================
# RATE LIMITING (requests per second, 0 = disabled)
# =============================================================================
# Protects server from overload. Returns 429 when exceeded.

# Global: Total requests/sec across all clients and projects
#   SMALL:  300   |  MEDIUM: 800   |  LARGE: 2000
RATE_LIMIT_REQUESTS_PER_SEC=800

# Per-IP: Requests/sec from single IP address
# Prevents single client from monopolizing resources
#   SMALL:  20    |  MEDIUM: 30    |  LARGE: 50
RATE_LIMIT_PER_IP_PER_SEC=30

# Per-Project: Requests/sec per project ID
# Prevents single project from affecting others
#   SMALL:  150   |  MEDIUM: 500   |  LARGE: 1000
RATE_LIMIT_PER_PROJECT_PER_SEC=500

# Burst multiplier: Allow brief bursts above limit
# Example: With limit=100 and multiplier=2, allows bursts up to 200
RATE_LIMIT_BURST_MULTIPLIER=2

# =============================================================================
# ANALYTICS & METRICS
# =============================================================================
# How often to flush aggregated metrics to database (seconds)
# Lower = more real-time, higher DB load
ANALYTICS_FLUSH_INTERVAL_SECS=10

# How long to keep analytics data (days)
# Older data is automatically deleted
ANALYTICS_RETENTION_DAYS=30

# Internal channel buffer size for metrics collection
# Formula: RPS × FLUSH_INTERVAL × 2 (with headroom)
# Higher = handles burst traffic better but uses more memory (~2-4MB for 20k)
#   SMALL:  10000 (300 RPS)  |  MEDIUM: 20000 (800 RPS)  |  LARGE: 50000 (2000 RPS)
ANALYTICS_BUFFER_SIZE=20000

# =============================================================================
# CACHING (automatically controlled by WORKER_INTERVAL_SECS)
# =============================================================================
# These caches reduce database load:
#   - Health stats: Refreshed every WORKER_INTERVAL_SECS seconds
#   - Project validation: TTL = WORKER_INTERVAL_SECS seconds
#
# No additional configuration needed.
